\chapter{Tests}

\section{Unit tests}
The \texttt{Scalatest} framework was used to perform some unit tests which drove the development of word and syllable splitting algorithms.

Namely, four \texttt{FunSuite}s were implemented in order to test:
\begin{itemize}
	\item the \texttt{CharacterSequence} trait behavior,
	\item the \texttt{Splitter} trait behavior,
	\item the syllabification of single words,
	\item the syllabification of entire verses.
\end{itemize}

Although complete coverage was not reached, test cases covered both common and exotic inputs, and all tests except two (caused by a failure of the chosen heuristics for those inputs) passed.

\section{Profiling}

In order to time the execution of the four proposed architectures, a \texttt{Profiler} trait was implemented and, in order to achieve higher granularity, their implementation was split into functions profiled separately.

The \texttt{Profiler} trait exposes two higher order methods: \texttt{profile[T](f: => T): (T, Long)} takes a function and times its execution (returning its return value and the time taken in nanoseconds), exploiting call-by-name parameter passing for a correct profiling, \texttt{smoothProfile[T](f: => T, times: Int): (T, Long)} profiles the function more than once and returns the average time (together with the last return value) in order to filter out noise caused by other processes running on the machine.

While optimization could not be turned off (many methods are tail recursive, therefore a compiler optimization is needed for large inputs), the tests have been performed on a fresh virtual machine (with no processes running other than the system daemons) mapped to 4 CPU cores (with $100 \%$ execution cap on the physical hardware) and $8 Gb$ of RAM, and averaged over multiple runs whenever applicable (neither the distributed architectures nor the tests on large inputs are averaged, the formers to avoid underestimating execution time due to caching, the latters due to lack of time).

An initial implementation used \texttt{List}s instead of \texttt{Queue}s, in order to make the code ``more functional'', however their poor performance (every append operation worked, especially using only the \texttt{::} operator, in $\mathcal{O}(n)$, while queues can \texttt{enqueue} in $\mathcal{O}(1)$) ultimately lead to ruling them out.
The following benchmarks are performed on both implementations, showing the dominance of performance of queues over lists also in practice, in terms of execution time.

The benchmarks were performed on the following files:
\begin{itemize}
	\item \texttt{La Divina Commedia.txt} composed of 561097 UTF-8 characters,
	\item \texttt{La Divina Commedia large.txt} composed of 2805485 UTF-8 characters (original dataset copy-pasted 5 times),
	\item \texttt{La Divina Commedia huge.txt} composed of 5610970 UTF-8 characters (original dataset copy-pasted 10 times),
	\item in memory strings generated by appending programmatically \texttt{La Divina Commedia.txt} to itself between 1 and 20 times.
\end{itemize}

\subsection{General assessment}

A preliminary assessment was performed in order to compare the performance of list-based and queue-based implementations.

The following is the output for lists (related only to the original dataset):
\begin{verbatim}
Profiling the sequential execution on file "La Divina Commedia.txt",
    split into 4 chunks (profiling averaged over 5 runs)...
Total (averaged) running time: 47921.384041 ms, of which:
  43.373189 ms for reading the file and splitting into chunks,
  47829.655373 ms for splitting each word into syllables and building
    a dictionary of occurrences for each chunk,
  35.288367 ms for merging the dictionaries into one,
  13.067112 ms for sorting and extracting the centiles.

Profiling the parallel execution on file "La Divina Commedia.txt",
    split into 4 chunks (profiling averaged over 5 runs).
    Each chunk is processed by a different thread...
Total (averaged) running time: 22331.325159 ms, of which:
  4.747231 ms for reading the file and splitting into chunks (sequential),
  22294.380319 ms for splitting each word into syllables and building a
    dictionary of occurrences for each chunk (parallel),
  23.938964 ms for merging the dictionaries into one (sequential),
  8.258645 ms for sorting and extracting the centiles (sequential).

Profiling the distributed execution on file "La Divina Commedia.txt",
    split into 4 chunks (profiling CANNOT be averaged).
    Each chunk is processed by a different node...
Total (averaged) running time: 28251.505847 ms, of which:
  2071.806305 ms for initializing Spark,
  455.901307 ms for reading the file and splitting into chunks (sequential),
  12477.873347 ms for splitting each word into syllables and building a
    dictionary of occurrences for each chunk (distributed),
  13245.924888 ms for sorting and extracting the centiles and shutting down
    Spark (sequential).

Profiling the distributed execution on file "La Divina Commedia.txt"
    with no prior splitting (profiling CANNOT be averaged).
    The number of nodes is chosen by Spark...
Total (averaged) running time: 87137.502624 ms, of which:
  94.868064 ms for initializing Spark,
  85781.084591 ms for reading the file, splitting into syllables and
    distributing the dataset (sequential),
  68.578017 ms for building a dictionary of occurrences (distributed),
  1192.971952 ms for sorting, extracting the centiles and shutting down
    Spark (sequential).
\end{verbatim}

As it can be noted, for every implementation, the slowest operation is the syllable splitting procedure, however a clear advantage is achieved when exploiting parallelism.
The number of pages the input is split into also relates to the performance of the merge procedure (slower when merging fewer occurrence maps of large size, instead of many smaller maps), this hints a non-linear, probably polynomial, complexity (since $m\cdot\mathcal{T}(n/m) \ll \mathcal{T}(n)$).


The following is the output for queues (on the same input file):
\begin{verbatim}
Profiling the sequential execution on file "La Divina Commedia.txt",
    split into 4 chunks (profiling averaged over 5 runs)...
Total (averaged) running time: 31229.42195 ms, of which:
  59.813815 ms for reading the file and splitting into chunks,
  31101.113841 ms for splitting each word into syllables and building a
    dictionary of occurrences for each chunk,
  54.814361 ms for merging the dictionaries into one,
  13.679933 ms for sorting and extracting the centiles.

Profiling the parallel execution on file "La Divina Commedia.txt",
    split into 4 chunks (profiling averaged over 5 runs).
    Each chunk is processed by a different thread...
Total (averaged) running time: 13635.397876 ms, of which:
  4.850249 ms for reading the file and splitting into chunks (sequential),
  13585.609283 ms for splitting each word into syllables and building a
    dictionary of occurrences for each chunk (parallel),
  35.490544 ms for merging the dictionaries into one (sequential),
  9.4478 ms for sorting and extracting the centiles (sequential).

Profiling the distributed execution on file "La Divina Commedia.txt",
    split into 4 chunks (profiling CANNOT be averaged).
    Each chunk is processed by a different node...
Total (averaged) running time: 19099.583035 ms, of which:
  2420.161538 ms for initializing Spark,
  240.389256 ms for reading the file and splitting into chunks (sequential),
  8209.232922 ms for splitting each word into syllables and building a
    dictionary of occurrences for each chunk (distributed),
  8229.799319 ms for sorting and extracting the centiles and shutting down
    Spark (sequential).

Profiling the distributed execution on file "La Divina Commedia.txt"
    with no prior splitting (profiling CANNOT be averaged).
    The number of nodes is chosen by Spark...
Total (averaged) running time: 76563.562394 ms, of which:
  171.860143 ms for initializing Spark,
  75096.973943 ms for reading the file, splitting into syllables
    and distributing the dataset (sequential),
  53.080434 ms for building a dictionary of occurrences (distributed),
  1241.647874 ms for sorting, extracting the centiles and shutting down
    Spark (sequential).
\end{verbatim}

The execution times are sensibly lower, demonstrating that the $\mathcal{O}(n)$ list append effectively acted as a bottleneck on most operations (including RDD transformations).

Further tests on the two implementations were run using the three datasets loaded from disk, in order to show the effect of varying input size.

\begin{figure}[H]
	\center
	\resizebox{\textwidth}{!}{
		\begin{tikzpicture}
		\begin{semilogyaxis}[axis lines=middle, xmin=0, xmax=10, ymin=0, ymax=3500,samples=1000, xtick={1,5,10}, xlabel={Dataset size}, ylabel={Running time (s)}, legend style={font=\tiny},
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},
		tick label style={font=\tiny},
		label style={font=\tiny},
		legend pos=outer north east,
		legend entries={Sequential, Parallel (4 thds), Parallel (20 thds), Spark 1 (4 nodes), Spark 1 (20 nodes), Spark 2}]
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			seql={mark=+, draw=black,mark size=2}
		}
		]
		file{data/seql.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			par4l={mark=o, draw=black,mark size=2}
		}
		]
		file{data/par4l.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			par20l={mark=diamond, draw=black,mark size=2}
		}
		]
		file{data/par20l.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark14l={mark=square, draw=black,mark size=2}
		}
		]
		file{data/spark14l.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark120l={mark=asterisk, draw=black,mark size=2}
		}
		]
		file{data/spark120l.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark2l={mark=triangle, draw=black,mark size=2}
		}
		]
		file{data/spark2l.txt};
		\addlegendimage{mark=+}
		\addlegendimage{mark=o}
		\addlegendimage{mark=diamond}
		\addlegendimage{mark=square}
		\addlegendimage{mark=asterisk}
		\addlegendimage{mark=triangle}
		\end{semilogyaxis}
		\end{tikzpicture}
	}
	\caption{Scalability of execution times on list-based implementations}
	\label{fig:listsemilog}
\end{figure}

\begin{figure}[H]
	\center
	\resizebox{\textwidth}{!}{
		\begin{tikzpicture}
		\begin{semilogyaxis}[axis lines=middle, xmin=0, xmax=10, ymin=0, ymax=3500,samples=1000, xtick={1,5,10}, xlabel={Dataset size}, ylabel={Running time (s)}, legend style={font=\tiny},
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},
		tick label style={font=\tiny},
		label style={font=\tiny},
		legend pos=outer north east,
		legend entries={Sequential, Parallel (4 thds), Parallel (20 thds), Spark 1 (4 nodes), Spark 1 (20 nodes), Spark 2}]
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			seqq={mark=+, draw=black,mark size=2}
		}
		]
		file{data/seqq.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			par4q={mark=o, draw=black,mark size=2}
		}
		]
		file{data/par4q.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			par20q={mark=diamond, draw=black,mark size=2}
		}
		]
		file{data/par20q.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark14q={mark=square, draw=black,mark size=2}
		}
		]
		file{data/spark14q.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark120q={mark=asterisk, draw=black,mark size=2}
		}
		]
		file{data/spark120q.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark2q={mark=triangle, draw=black,mark size=2}
		}
		]
		file{data/spark2q.txt};
		\addlegendimage{mark=+}
		\addlegendimage{mark=o}
		\addlegendimage{mark=diamond}
		\addlegendimage{mark=square}
		\addlegendimage{mark=asterisk}
		\addlegendimage{mark=triangle}
		\end{semilogyaxis}
		\end{tikzpicture}
	}
	\caption{Scalability of execution times on queue-based implementations}
	\label{fig:queuesemilog}
\end{figure}

For both implementations, it can be noted how the second Spark implementation (minimum inter-node communication) scales the worst, followed by low-parallelism implementations (parallel architecture and the first Spark implementation with few nodes).
The best scalability is achieved by the sequential implementation and high-parallelism implementations, hinting the fact that for increasing datasets, the occurrence map merging operation becomes more and more relevant for the overall performance (the sequential implementation doesn't need to merge anything, while high-parallelism implementations need to merge only small-sized maps).

\subsection{Execution time with respect to degree of parallelism}
In order to further investigate the dependency of execution times with respect to the degree of parallelism, the parallel architecture was tested using a fixed input (the original dataset), varying the number of threads.

\begin{figure}[H]
	\center
	\resizebox{\textwidth}{!}{
		\begin{tikzpicture}
		\begin{axis}[axis lines=middle, xmin=0, xmax=100, ymin=0, ymax=70,samples=1000, xtick={0,10,...,100}, xlabel={Number of threads}, ylabel={Running time (s)}, legend style={font=\tiny},
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},
		tick label style={font=\tiny},
		label style={font=\tiny},
		legend pos=outer north east,
		legend entries={Using lists, Using queues}]
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			lists={mark=+,draw=black,mark size=2}
		}
		]
		file{data/threads_lists.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			queues={mark=o, draw=black,mark size=2}
		}
		]
		file{data/threads_queues.txt};
		\addlegendimage{mark=+}
		\addlegendimage{mark=o}
		\end{axis}
		\end{tikzpicture}
	}
	\caption{Efficiency with respect to the number of threads}
	\label{fig:threads}
\end{figure}

It is important to notice that JVM's mapping is many-to-many, so the number of threads effectively running simultaneously will depend on the system's load, however it will never exceed the number of CPUs available on the machine (4 in this case).
As such, the expected performance (on the parallel component only) should converge pretty soon (and after a given threshold context-switching overhead should cause a performance degradation).

Although this convergence can be clearly seen on the plot, it doesn't happen as soon as expected, due to the combination with the performance of the sequential part of the implementation (namely the occurrence maps merging, which, as stated before, performs better when merging many smaller maps instead of few larger ones).

Finally, as already demonstrated, the list-based implementation is in all cases outperformed by the queue-based one.

\subsection{Comparing the most promising architectures}
A final test was performed comparing the most promising architectures (sequential, parallel implementation with 20 threads and Spark implementation with maximal parallelism using 20 nodes) on the queue-based implementation to better assess the scalability.

The input was given a a string obtained by repeatedly concatenating in RAM the original dataset a given number of times, due to this lack of I/O operations and the fact that the complete architectures were profiled in a single function for each of them, compiler optimization achieved different results than the other tests (element-wise figures in this test are smaller than the ones obtained in other tests), however the general trend is preserved.

The sequential implementation clearly diverges from the other two, the larger the input becomes, while the parallel implementations tend to perform similarly (this is justified by the fact that on a single machine Spark will still be subjected to the same hardware limitations as the multithreaded implementation, and due to the fact that its core is not completely shut down between each experiment, leading to warm-restarts).

For clarity, both a linear and a semilog plot of the same results are shown, from both a polynomial trend clearly emerges.

\begin{sidewaysfigure}
	\center
	\resizebox{\textwidth}{!}{
		\begin{tikzpicture}
		\begin{axis}[axis lines=middle, xmin=0, xmax=20, ymin=0, ymax=3500,samples=1000, xtick={1,5,10,15,20}, xlabel={Dataset size}, ylabel={Running time (s)}, legend style={font=\tiny},
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},
		tick label style={font=\tiny},
		label style={font=\tiny}]
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			seqq={mark=+, draw=black,mark size=2}
		}
		]
		file{data/seqq_2.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			par20q={mark=diamond, draw=black,mark size=2}
		}
		]
		file{data/par20q_2.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark120q={mark=asterisk, draw=black,mark size=2}
		}
		]
		file{data/spark120q_2.txt};
		\end{axis}
		\end{tikzpicture}
		\begin{tikzpicture}
		\begin{semilogyaxis}[axis lines=middle, xmin=0, xmax=20, ymin=0, ymax=3500,samples=1000, xtick={1,5,10,15,20}, xlabel={Dataset size}, ylabel={Running time (s)}, legend style={font=\tiny},
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south},
		tick label style={font=\tiny},
		label style={font=\tiny},
		legend pos=outer north east,
		legend entries={Sequential, Parallel (20 thds), Spark 1 (20 nodes)}]
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			seqq={mark=+, draw=black,mark size=2}
		}
		]
		file{data/seqq_2.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			par20q={mark=diamond, draw=black,mark size=2}
		}
		]
		file{data/par20q_2.txt};
		\addplot[
		ultra thin,
		scatter,
		point meta=explicit symbolic,
		forget plot,
		scatter/classes={
			spark120q={mark=asterisk, draw=black,mark size=2}
		}
		]
		file{data/spark120q_2.txt};
		\addlegendimage{mark=+}
		\addlegendimage{mark=diamond}
		\addlegendimage{mark=asterisk}
		\end{semilogyaxis}
		\end{tikzpicture}
	}
	\caption{Running time of the queue-based implementation}
	\label{fig:topolino}
\end{sidewaysfigure}